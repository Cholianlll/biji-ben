# STAT3015/4030/STAT7030 GLMs
# R command file for Tutorial 4

# Question 1
house <- read.table("House.txt", header = TRUE)
house

# (a)
# Fit a parallel regression ANCOVA model
house.lm <- lm(Price ~ Age + Taxes + LivingSpace + LotSize + factor(Bedrooms), data = house)
anova(house.lm)

# Check diagnostics
# Can also use plot(house.lm)
plot(house.lm, which = 1)
# Identify the two odd points in the Residuals vs Fitted plot
identify(house.lm$fitted.values, house.lm$residuals)

plot(house.lm, which = 2)
plot(house.lm, which = 5)

# What's unusual with the flagged out observations 9, 10 and 27
house

# Investigate further removing this data point has a 
# dramatic effect on the parameter estimates
house.lm1 <- lm(Price ~ Age + Taxes + LivingSpace + LotSize + factor(Bedrooms), 
                data = house[-27,])
coef(house.lm)
coef(house.lm1)

# (b)
# A 95% CI for the expected difference in price between a three and a hour-bedroom house
attach(house)
bed3 <- ifelse(Bedrooms==3, 1, 0)
bed4 <- ifelse(Bedrooms==4, 1, 0)
bed5 <- ifelse(Bedrooms==5, 1, 0)
beds <- cbind(bed3, bed4, bed5)
house.reg<-lm(Price ~ Age + Taxes + LivingSpace + LotSize + beds, data = house)
cc <- c(0, 0, 0, 0, 0, -1, 1, 0)
diff34 <- as.vector(t(cc)%*%house.reg$coef)
Xmat <- cbind(1, Age, Taxes, LivingSpace, LotSize, beds)
XtXi <- solve(t(Xmat)%*%Xmat)
sqrtMSE <- summary(house.reg)$sigma
sd <- sqrtMSE*sqrt(as.vector(t(cc)%*%XtXi%*%cc))
upper <- diff34 + (qt(0.975, house.reg$df.residual)*sd)
lower <- diff34 - (qt(0.975, house.reg$df.residual)*sd)
cbind(lower, diff34, upper)
# The confidence intervals contain the value 0. 
# This means that there is no apparent difference in price between
# a three and four bedroom house.

cc <- c(0, 0, 0, 0, 0, 0, -1, 1)
diff45 <- as.vector(t(cc)%*%house.reg$coef)
sd <- sqrtMSE*sqrt(as.vector(t(cc)%*%XtXi%*%cc))
upper <- diff45 + (qt(0.975, house.reg$df.residual)*sd)
lower <- diff45 - (qt(0.975, house.reg$df.residual)*sd)
cbind(lower, diff45, upper)
# There is a large difference (nearly $20,000) between 
# a four and a five bedroom house.

# (c)
# Correlation matrix of the predictors
Xmat <- cbind(Age, Taxes, LivingSpace, LotSize, beds)
cor(Xmat)
# Notice that there are some large correlations between the continuous predictors and the
# indicators for the number of bedrooms. This means that multicollinerity may be a problem.
# In addition, it may partly explain the reason for the strange result in part (b) whereby
# three and four bedroom houses seemed to have the same price.

# (d)
# Stepwise variable selection
step(house.lm)

# Perform a cross-validation to assess the predictive capability of the selected model
# 1 to 20 observations are trainging set
# 21 to 27 observations are testing set
Xmat.cross <- cbind(1, Taxes, LivingSpace, beds)
reg.cross <- lm(Price[1:20] ~ Taxes[1:20] + LivingSpace[1:20] + beds[1:20,])
sum((Price[21:27] - (Xmat.cross[21:27,]%*%coef(reg.cross)))^2)

# Cross-validation for model in (a)
Xmat.cross2 <- cbind(Age, Taxes, LivingSpace, LotSize, beds)
reg.cross2 <- lm(Price[1:20] ~ Age[1:20] + Taxes[1:20] + LivingSpace[1:20] 
                 + LotSize[1:20] + beds[1:20,])
sum((Price[21:27] - (coef(reg.cross2)[1] + Xmat.cross2[21:27,]%*%coef(reg.cross2)[-1]))^2)
# It seems that the smaller model is better from this perspective.


# Question 2
# This example uses the teacher effectiveness example (example 2) from pages 13 to 18 of the 
# brick. A lot of the old S-Plus code shown in the brick still works in R.

# First the S-Plus code and model shown in the brick at bottom of page 13 and at the top of
# page 14:

tchreff <- read.table("TeachEff.txt",header=T)
attach(tchreff)
names(tchreff)
tchreff.lm <- lm(teff ~ x1 + x2 + x3 + x4 + factor(gender))
anova(tchreff.lm)

# Some differences in rounding from the ouput shown in the brick. There are also small changes
# in the format of the output - there are many such small differences between R and S-Plus, 
# reflecting the fact that different people with different personal preferences created the 
# functions in the two languages! 

# We can also reproduce the table shown on page 14 of the brick using some "user-defined" 
# model selection functions that were created in the STAT2008/STAT6038 Regression Modelling 
# course - though this is definitely a topic from that prerequisite course, not a required
# part of this current course.

# Two updated model selection functions (originally written in S-Plus) from
# STAT2008/STAT6038 Regression Modelling course:

selection.criteria <- function(model, ideal_mse=I(summary(model)$sigma^2)) {
  parameters <- model$rank
  sample.size <- parameters + model$df.residual
  leverages <- hatvalues(model)
  deletion.residuals <- residuals(model)/(1-leverages)
  mse <- summary(model)$sigma^2
  pressp <- sum(deletion.residuals^2)
  r.squared <- summary(model)$r.squared
  adj.r.squared <- summary(model)$adj.r.squared
  Cp <- parameters + (((sample.size-parameters)*(mse-ideal_mse))/ideal_mse)
  temp <- cbind(parameters, mse, pressp, r.squared, adj.r.squared, Cp)
  dimnames(temp) <- list(paste(model$call)[2],c("p","MSE","PRESSp","R-Squared","Adj.R-Sqd","Cp"))
  temp
}

model.selection <- function(response, predictors, model.list=matrix(1:ncol(predictors),nrow=1)) {
  full.model <- lm(response ~ predictors)
  full.mse <- summary(full.model)$sigma^2
  temp <- model.list
  dimnames(temp) <- list(dimnames(model.list)[[1]],dimnames(predictors)[[2]])
  result <- cbind(temp,p=0,MSE=0,PRESSp=0,"R-Squared"=0,"Adj.R-Sqd"=0,"Cp"=0)
  for (i in (1:nrow(model.list))) {
    variables <- model.list[i,]
    for (j in (1:length(variables))) {
      ifelse(variables[j]>0,variables[j]<-j,variables[j]<-0)
    }
    part.model <- lm(response ~ predictors[,variables])
    result[i,(ncol(predictors)+1):ncol(result)] <- selection.criteria(part.model, ideal_mse=full.mse)
  }
  as.data.frame(result)
}

# The model selection function calls the selection criteria function to produce various model
# diagnostics for a series of models. It expects as input the response variable as a vector and 
# the explanatory variabels in a matrix (if you want indicator, factor, polynomial or interaction
# terms, you need to first create the appropriate variables and include them in the matrix):

# Now to apply these functions to the teacher effectiveness data:

unique(gender)
levels(gender)
gndr <- ifelse(gender=="M",0,1)
tchreff.all <- cbind(gndr, x1, x2, x3, x4, teff)

# The model selection function also expects a list of models, where each model is described 
# by a vector listing which columns of the explanatory data matrix are to be included in the
# model:

models <- rbind(
  c(1,0,0,0,0),
  c(1,1,0,0,0),
  c(1,0,1,0,0),
  c(1,0,0,1,0),
  c(1,0,0,0,1),
  c(1,1,1,0,0),
  c(1,1,0,1,0),
  c(1,1,0,0,1),
  c(1,0,1,1,0),
  c(1,0,1,0,1),
  c(1,0,0,1,1),
  c(1,1,1,1,0),
  c(1,1,1,0,1),
  c(1,1,0,1,1),
  c(1,0,1,1,1),
  c(1,1,1,1,1))

# Now we are ready to reproduce the table on page 14:

bigtable <- model.selection(teff,tchreff.all[,1:5],models)
bigtable

# Gender should be included in the model, as the research question is about whether or not the 
# response variable (teacher effectiveness) differs between the sexes. The above suggests our
# model should also include x2 and x4 as explanatory variables.

# Yet another approach, suggested at the bottom of page 14 of the brick, is to use forward 
# stepwise regression.

# forstp <- stepwise(tchreff.all[,1:5],tchreff.all[,6],f.crit=c(3.5,3))
# forstp$which

# In R, no-one has yet written a function called stepwise(), so the above S-Plus code doesn't
# work!

# There is also a function called step() available in both S-Plus and R - see the help files
# for details!

# Here is the the default version of forward selection in R version:

step(lm(teff ~ ., data=data.frame(tchreff.all)))

# This suggests dropping gender (which we can't do) and keeping all four of the other x's.

step(lm(teff ~ ., data=data.frame(tchreff.all)))

# This function uses a completely different selection criteria (AIC) and has to be
# "tweaked", otherwise it results in a totally different model from the one suggested by 
# stepwise(): 

lm.start <- lm(teff ~ gndr + x1 + x2 + x3 + x4, data=as.data.frame(tchreff.all))
step(lm.start)
step(lm.start, k=4)
lm.final <- step(lm.start, k=4)
anova(lm.final)

# So, both stepwise() in S-Plus and the step() function in R , suggest a model without gndr, 
# but if we have an a priori conviction that gender is likely to cause a difference in teff,
# the only way we can test this assumption is to include it in our models.

# We can get close to the model suggested in the brick using stepwise refinement instead of
# forward selection and limiting the scope to at least initially include gender

step(lm(teff ~ gender), data=data.frame(tchreff.all), scope=list(upper=as.formula("teff ~ gender + x1 + x2 + x3 + x4")), direction="both")

# This at least arrives temporarily at the model with gender, x2 and x4, before deciding that
# the model would be better off without gender.

anova(lm(teff ~ x2 + x4 + factor(gender)))
anova(lm(teff ~ factor(gender) + x2 + x4))

# Note, we do not have orthogonal contrasts in this observational data, so it does make a 
# difference which order we include the explanatory variables in the model.

# Gender does not appear to be significant, regardless of how we include it in the model!
# This suggests that we really need only a single multiple regression model, rather than 
# two parallel models, one for each of the two genders! 

# However x1, x2, x3 and x4 are obviously all inter-related and there is a faint possibility 
# that the relationship between x2 and x4 is masking an effect of gender on teacher
# effectiveness in the above model. The next section of the brick (on pages 17 and 18), applies
# some more multiple regression techniques from STAT2008 to investigate this
# possibility by examining added variable plots for x2 and x4 in the context of the above model.

# The following code from the brick produces two partial residual plots to further explore any
# possible inter-relationships between x2, x4 and gender. "Automatic" model selection, stepwise
# model refinement and partial regression plots were all topics in STAT2008/STAT6038, which is 
# a course on multiple regression modelling.

# These techniques all play in role in statistical modelling in general, including generalised
# linear modelling (especially in the area of data reduction of large and complicated datasets);
# however, they were covered in the earlier prerequisite course and are not examinable topics
# in this course.

rsd1 <- residuals(lm(x2 ~ factor(gender) + x4))
rsd2 <- residuals(lm(teff ~ factor(gender) + x4))
plot(rsd1,rsd2,type="n",xlab="e(x2|x4,gender)", ylab="e(teff|x4,gender)")
points(rsd1[gender=="M"],rsd2[gender=="M"],pch="M",cex=0.7)
points(rsd1[gender=="F"],rsd2[gender=="F"],pch="F",cex=0.7)
abline(lsfit(rsd1[gender=="M"],rsd2[gender=="M"])$coef,lty=2)
abline(lsfit(rsd1[gender=="F"],rsd2[gender=="F"])$coef,lty=3)

rsd1 <- residuals(lm(x4 ~ factor(gender) + x2))
rsd2 <- residuals(lm(teff ~ factor(gender) + x2))
plot(rsd1,rsd2,type="n",xlab="e(x4|x2,gender)", ylab="e(teff|x2,gender)")
points(rsd1[gender=="M"],rsd2[gender=="M"],pch="M",cex=0.7)
points(rsd1[gender=="F"],rsd2[gender=="F"],pch="F",cex=0.7)
abline(lsfit(rsd1[gender=="M"],rsd2[gender=="M"])$coef,lty=2)
abline(lsfit(rsd1[gender=="F"],rsd2[gender=="F"])$coef,lty=3)

# See the discussion of these plots on page 17 of the brick.

# Finally, could our earlier analysis of the effects of gender be incomplete and do we really
# need (rather than a single multiple regression or parallel regression models for the 
# two genders), completely separate regression models for the two genders?:

anova(lm(teff ~ gndr + x2 + x4 + gndr:x2 + gndr:x4))

# This suggests we don't need either of the two interaction terms! We can confirm this using
# the following code from the brick, which provides a single test for the addition of these
# two terms:

Zx2 <- gndr*x2
Zx4 <- gndr*x4
Zx <- cbind(Zx2,Zx4)
eff.lm <- lm(teff ~ x2 + x4 + gndr + Zx)
anova(eff.lm)

# So, we also find that (even combining the two interactions together) there is no significant
# effect, suggesting that the above models are not masking any hidden effects and we really
# can conclude that that gender does NOT have an effect on teacher effectiveness.

# In short, our assumption of any gender effect appears to be wrong or, at least, it is not
# substantiated by these data!

