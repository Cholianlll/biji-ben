\documentclass[10pt]{article}
\usepackage{fullpage}
%\usepackage{amsmath}
%\usepackage{mathtools}
%\usepackage{setspace}
\usepackage{listings}
%\doublespacing
\usepackage[parfill]{parskip}
\overfullrule=2cm
\begin{document}
%Problem 1%
\section*{Problem 1}
\subsection*{(a)}
Below is the R-code for part (a)
\begin{lstlisting}[language=R,frame=single]
#prior parameters
mu0<-7 ; g20<-5
t20<-10; eta0<-2
s20<-15; nu0<-2
#

#starting values
m<-length(school)
n<-sv<-ybar<-rep(NA,m)
for(j in 1:m){
  ybar[j]<-mean(school[[j]])
  sv[j]<-var(school[[j]])
  n[j]<-length(school[[j]])
}
theta<-ybar    ; sigma2<-mean(sv)
mu<-mean(theta); tau2<-var(theta)
#

#setup MCMC
set.seed(1)
S<-5000
THETA<-SMT<-NULL
#

#MCMC algorithm
for(s in 1:S){
  
  #sample new value of the thetas
  for(j in 1:m){
    vtheta<-1/(n[j]/sigma2+1/tau2)
    etheta<-vtheta*(ybar[j]*n[j]/sigma2+mu/tau2)
    theta[j]<-rnorm(1,etheta,sqrt(vtheta))
  }
  
  #sample a new value of sigma2
  nun<-nu0+sum(n)
  ss<-nu0*s20
  for(j in 1:m){ss<-ss+sum((school[[j]]-theta[j])^2)}
  sigma2<-1/rgamma(1,nun/2,ss/2)
  
  #sample a new value of mu
  vmu<-1/(m/tau2+1/g20)
  emu<-vmu*(m*mean(theta)/tau2+mu0/g20)
  mu<-rnorm(1,emu,sqrt(vmu))
  
  #sample a new value of tau2
  etam<-eta0+m
  ss<-eta0*t20+sum((theta-mu)^2)
  tau2<-1/rgamma(1,etam/2,ss/2)
  
  #store results
  THETA<-rbind(THETA,theta)
  SMT<-rbind(SMT,c(sigma2,mu,tau2))
  
}
\end{lstlisting}
\par
From the output below, we see that the effective sample sizes are all much larger than 1000.
\begin{lstlisting}[language=R,frame=single]
> for(i in 1:m){cat(effectiveSize(THETA[,i]),"\n")}
4551.612 
4968.519 
5000 
5465.115 
4096.163 
4625.084 
5576.988 
4776.939 
> for(i in 1:3){cat(effectiveSize(SMT[,i]),"\n")}
4739.021 
4177.568 
3618.343 
\end{lstlisting}
\pagebreak
\subsection*{(b)}
We can approximate the posterior mean and 95\% confidence interval of $\sigma^2$, $\mu$, and $\tau^2$ using the MCMC samples:
\begin{lstlisting}[language=R,frame=single]
> apply(SMT,2,mean)
  sigma2       mu     tau2 
1.010420 7.553082 5.659925 
> apply(SMT,2,function(x) quantile(x,prob=c(0.025,0.975)))
         sigma2       mu      tau2
2.5%  0.8203852 5.992710  2.064836
97.5% 1.2387424 9.077109 14.550425
\end{lstlisting}
\par
We then compare it to the 95\% confidence intervals of prior distributions of $\sigma^2$, $\mu$, and $\tau^2$:
\begin{lstlisting}[language=R,breaklines,frame=single,keepspaces=true]
> #prior 95% CI
> sigma2.quantile.prior<-c(qigamma(0.025,nu0/2,nu0*s20/2),qigamma(0.975,nu0/2,nu0*s20/2))
> mu.quantile.prior<-qnorm(c(0.025,0.975),mu0,g20)
> tau2.quantile.prior<-c(qigamma(0.025,eta0/2,eta0*t20/2),qigamma(0.975,eta0/2,eta0*t20/2))
> sigma2.quantile.prior
[1]  4.066275 592.468353
> mu.quantile.prior
[1] -2.79982  16.79982
> tau2.quantile.prior
[1]  2.71085  394.97890
\end{lstlisting}
\par
It is noticed that the posterior confidence interval in much narrower compared with the prior confidence interval. The 95\% posterior confidence interval of $\sigma^2$ does not even overlap with the 95\% prior confidence interval. The data provides us information to make more precise and accurate estimates of the parameters we are interested in.


\end{document}
