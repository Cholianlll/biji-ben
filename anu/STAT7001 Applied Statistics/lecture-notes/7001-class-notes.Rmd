---
output: 
    pdf_document:
        latex_engine: xelatex
fontsize: 8pt
geometry: margin=0.1in
mainfont: RobotoCondensed-Light
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path='Figs/', dev = 'png', out.width='320px', 
                      out.height='200px',dpi=200,fig.align = "center",
                      small.mar=TRUE,
                      echo=TRUE, warning=FALSE, message=FALSE)
```

**1. Simple Linear Regression and Its Estimation** **1.1 Introduction to SLR** $\blacktriangledown$ Regression: mathematical relationship between the mean of the response variable and the explanatory variable. $\mu\{Y\mid X\}$: the regression of $Y$ on $X$ = the mean of $Y$ as a function of $X$. $\blacktriangledown$$\sigma\{Y\mid X\}$: the standard deviation of $Y$ as a function of $X$. $\blacktriangledown$Particular form of SLR: $\mu\{Y\mid X\}=\beta_0+\beta_1 X$ where $\beta_0$ is the mean $Y$ when $X$ takes $0$, $\beta_1$ is the increase in the mean of $Y$ per one-unit increase in $X$. $\beta_0,\beta_1$ unknown in the model. **1.2 SLR Model Assumptions** $\blacktriangledown$**Linearity**: The means of the populations fall on a straight-line function of the explanatory variable. $\blacktriangledown$**Normality**: There is a normally distributed population of responses for each value of the explanatroy variable. $\blacktriangledown$**Constant variance**: The population standard deviations are all equal: $\sigma\{Y\mid X\}=\sigma$. $\blacktriangledown$**Independence**: $(X_i,Y_i)$'s are independent of each other, where $i$ is a positive integer no greater than sample size $n$. $\blacktriangledown$Note: $Y=\mu\{Y\mid X\}+\epsilon$, where $\epsilon\sim N(0,\sigma^2)$, i.e. $Y\sim N(\mu\{Y\mid X\},\sigma^2)$. **1.3 Estimation of SLR Model** $\blacktriangledown$"Least Squares" method. $\blacktriangledown$"Best fitting" $\hat{\beta}_0$ and $\hat{\beta}_1$. $\blacktriangledown$Key step is to minimize $Q(b_1,b_0)=\sum^n_{i=1}(Y_i-b_0-b_1X_i)^2$, then we have $\hat{\beta}_1=b_1=\frac{\sum^n_{i=1}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum^n_{i=1}(X_i-\bar{X})^2},\hat{\beta}_0=b_0=\bar{Y}-\hat{\beta}_1\bar{X}.$ $\blacktriangledown$Estimates are unbiased, $E(\hat{\beta}_k)=\beta_k,k=1,0.$ $\blacktriangledown$Estimated mean function $\hat{\mu}\{Y\mid X\}=\hat{\beta}_0+\hat{\beta}_1X$ $\blacktriangledown$Fitted/predicted value: $\hat{Y}_i=\hat{\mu}\{Y_i\mid X_i\}=\hat{\beta}_0+\hat{\beta}_1X_i$. $\blacktriangledown$Residuals: $\hat{\epsilon}_i=Y_i-\hat{Y}_i$ 

**2. Inferential Tools for SLR** **2.1 Sampling Distribution of Estimation** $\blacktriangledown$Different data sets give different realizations of $\hat{\beta}_0$ and $\hat{\beta}_1$. The distributions of the realizations are the sampling distributions. $\blacktriangledown$Sampling distributions of $\hat{\beta}_0$ and $\hat{\beta}_1$ are both **normal**. $\blacktriangledown$ $\hat{\beta}_1\sim N\left(\beta_1,\frac1{(n-1)s_X^2}\sigma^2\right),\hat{\beta}_0\sim N\left(\beta_0,\left(\frac1n+\frac{\bar{X}^2}{(n-1)s_X^2}\right)\sigma^2\right),s_X^2=\frac1{n-1}\sum^n_{i=1}(X_i-\bar{X})^2$ $\blacktriangledown$Simulation. **2.2 Standard Error of Estimation** $\blacktriangledown$Recall $SD(\hat{\beta}_1)=\sigma\sqrt{\frac1{(n-1)s_X^2}},SD(\hat{\beta}_0)=\sigma\sqrt{\frac1n+\frac{\bar{X}^2}{(n-1)s_X^2}}$. However, for a real dataset, $\sigma$ is unknown, but we can estimated by $\hat{\sigma}=\sqrt{\frac{\sum^n_{i=1}\text{res}_i^2}{n-2}},\text{res}_i=Y_i-\hat{\beta}_0-\hat{\beta}_1X_i$. $\blacktriangledown$$n-2$ is the degrees of freedom, s.t. $E(\hat{\sigma}^2)=\sigma^2$. Therefore, the estimator standard errors $SE(\hat{\beta}_1)=\hat{\sigma}\sqrt{\frac1{(n-1)s_X^2}},SE(\hat{\beta}_0)=\hat{\sigma}\sqrt{\frac1n+\frac{\bar{X}^2}{(n-1)s_X^2}}$ **2.3 Hypothesis Testing** $\blacktriangledown$Another form (practical sampling distribution) for estimators is $\frac{\hat{\beta}_k-\beta_k}{SD(\hat{\beta}_k)}\sim N(0,1), k=0,1, \text{ but } SD(\hat{\beta}_k) \text{ is unknown.},\frac{\hat{\beta}_k-\beta_k}{SE(\hat{\beta}_k)}\sim t_{n-2},k=0,1, \text{ where } SE(\hat{\beta}_k) \text{ is known.}$ $\blacktriangledown$$H_0:\beta_k=0$ vs. $H_a:\beta_k\not=0$, test statistics=$TS=\frac{\hat{\beta}_k-0}{SE(\hat{\beta}_k)}.$ $\blacktriangledown$p-value = $2\times P(T>|TS|)$, where $T\sim t_{n-2}$. $\blacktriangledown$p-value < predetermined significance level $\alpha\implies TS$ falls into the two tails of the $t$ distribution $\implies |TS|$ is too large $\implies$ Reject $H_0$. $\blacktriangledown$correlation $\not=$ causation; confounding variables; mlr. **2.4 Confidence Intervals and Prediction Intervals**
$\blacktriangledown$$(1-\alpha)$ CI for $\beta_k: \hat{\beta}_k\mp t_{n-2,\alpha/2}\times SE(\hat{\beta}_k)$ $\blacktriangledown$$(1-\alpha)$ CI for mean of response $\mu\{Y\mid X=x_0\}=\beta_0+\beta_1x_0$ is $(\hat{\beta}_0+\hat{\beta}_1x_0)\mp t_{n-2,\alpha/2}\times SE(\hat{\beta}_0+\hat{\beta}_1x_0)$, where $SE(\hat{\beta}_0+\hat{\beta}_1x_0)=\hat{\sigma}\sqrt{\frac1n+\frac{(x_0-\bar{X})^2}{(n-1)s_X^2}}$ $\blacktriangledown$$(1-\alpha)$ PI for a future response $Y_{new}$ at $X_{new}$ is $(\hat{\beta}_0+\hat{\beta}_1X_{new})\mp t_{n-2,\alpha/2}\times SE\{(\hat{\beta}_0+\hat{\beta}_1X_{new})-Y_{new}\}, SE\{(\hat{\beta}_0+\hat{\beta}_1X_{new})-Y_{new}\}=\hat{\sigma}\sqrt{1+\frac1n+\frac{(X_{new}-\bar{X})^2}{(n-1)s_X^2}}.$

**3. Model Diagnostics for Linear Regression I** **3.1 Incentive** 4 assumptions!
$\blacktriangledown$**Violations of Linearity**: Can cause the estimated means and predictions to be biased. $\blacktriangledown$**Violations of Normality**: Coefficient estimates are robust to some non-normal distributions. $\blacktriangledown$**Violations of Constant Variance**: Standard errors may inaccurately measure uncertainty. $\blacktriangledown$**Violations of Independece**: Can seriously affect standard errors. **3.2 Graphical Tools for Model Diagnostics** **3.2.1 Response vs explanatory variable** $\blacktriangledown$transform **3.2.2 Residuals vs fitted** $\blacktriangledown$expect a rectangular pattern around zero-line **3.2.3 Normal QQ** $\blacktriangledown$Plots the ordered observed residuals vs what we would expect for these values if the residuals were normally distributed.

**4. MLR and Its Estimation** **4.1 Intro** $\blacktriangledown$$\mu\{Y\mid X\}=\beta_0+\beta_1 X_1+\cdots +\beta_k X_k$, where $X=(X_1,\cdots, X_k)$, $\mu\{Y\mid X\}$ as the regression of $Y$ on $X$, $\sigma\{Y\mid X\}$ the standard deviation of $Y$ as a function of $X$. $\blacktriangledown$"Linear" refers to the regression coefficients, so a MLR model can include higher integer order model term of predictors. $\blacktriangledown$marginal effect of predictor (other predictors held constant) **4.2 MLR Model Assumptions** $\blacktriangledown$Linearity, normality, constant variance, independence. **4.3 Estimation of MLR Model** $\blacktriangledown$LS estiamtes of $\hat{\beta}_0$ and $\hat{\beta}_1$ of SLR are $(\hat{\beta}_0,\hat{\beta}_1,\cdots,\hat{\beta}_k)^T=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{Y}$ where the $n\times(k+1)$ design matrix ($n$ rows, $(k+1)$ columns) with $\mathbf{1}$ as first column, $(X_{i,1},\cdots,X_{i,n})^T$ as i-th column vector and $\textbf{Y}=(Y_1,Y_2,\cdots,Y_n)^T$.

**5. MLR for Categorical Explanatory Variables** **5.1 Continuous and Categorical Data** $\blacktriangledown$Continous $Y$+Continuous $X$ (MLR) $\blacktriangledown$ Continous$Y$ + Categorical $X$ (MLR+Indicator) $\blacktriangledown$ Categorical $Y$+Continuous $X$ (Logistic) $\blacktriangledown$Categorical $Y$+Categorical $X$ (Logistic+Indicator) **5.2 Indicator Variables** $\blacktriangledown$$k$ categories will be represented by only $k-1$ indicator variables otherwise would cause multicolinearity. $\blacktriangledown$baseline level **5.3 Interaction** $\blacktriangledown$interpretation of interaction term always involves discussion by cases. $\blacktriangledown$Even though the coefficients of an interaction and one related variable are not significant, we have no strong evidence to set them 0 directly. Also, if we remove these two terms, the model is no longer "best fit", needs to be refitted.

**6. Inferential Tools for MLR** **6.1 Sampling Distribution of Estimation** $\blacktriangledown$$Y\sim N(\beta_0+\beta_1 X_1+\cdots +\beta_k X_k,\sigma^2)$, the sampling distribution for $\hat{\beta}_j$ can be described by $\frac{\hat{\beta}_j-\beta_j}{SD(\hat{\beta}_j)}\sim N(0,1), \text{ where} SD(\hat{\beta}_j)=\sigma\sqrt{e^T_{j+1}(\textbf{X}^T\textbf{X})^{-1}e_{j+1}}, \text{ and} e_{j+1}=(0,\cdots,0,1,0,\cdots,0)^T, \text{ is a } (k+1)\times 1 \text{ vector.}$ **6.2 Standard Error of Estimation** $\blacktriangledown$$\sigma$ is unknown, but we can estimate it by $\hat{\sigma}=\sqrt{\frac{\sum^n_{i=1}\text{res}^2_i}{n-k-1}}$ $\blacktriangledown$$n-k-1$ is the number of degrees of freedom s.t. $E(\hat{\sigma}^2)=\sigma^2.$  $\blacktriangledown$$k+1$ is the number of regression coefficients. $\blacktriangledown$plut it in, consequently we have $SD(\hat{\beta}_j)=\sigma\sqrt{e^T_{j+1}(\textbf{X}^T\textbf{X})e_{j+1}},SE(\hat{\beta}_j)=\hat{\sigma}\sqrt{e^T_{j+1}(\textbf{X}^T\textbf{X})e_{j+1}}$ **6.3 Hypothesis Testing** $\blacktriangledown$practical sampling distribution: $\frac{\hat{\beta}_j-\beta_j}{SE(\hat{\beta}_j)}\sim t_{n-k-1},\forall j=0,\dots,k.$ **6.3.1 t-Test** $\blacktriangledown$$H_0:\beta_j=0$ vs $H_a:\beta_j\not=0.$ $\blacktriangledown$Test Statistics = $TS= \frac{\hat{\beta}_j-0}{SE(\hat{\beta}_j)}$ $\blacktriangledown$p-value=$2\times P(T>|TS|)$, where $T\sim t_{n-k-1}$. $\blacktriangledown$If p-value < $\alpha\implies$ reject $H_0$; p-value $\geq \alpha\implies$ not reject $H_0$. **6.3.2 F-Test** $\blacktriangledown$**The meaning of the coefficient of an explanatory variable depends on what other explanatory variables have been included in the regression.** $\blacktriangledown$F-test avoids the problem when variables are highly correlated. $\blacktriangledown$$H_0:$ none of $X_i$'s are needed in the model, $\beta_1=\beta_2=\cdots=\beta_k=0.$ $\blacktriangledown$$H_a:$ at least one of $X_i$'s is needed, at least one of $\beta_i\not=0.$ $\blacktriangledown$F-test is used to test whether or not a subgroup of $\beta_j, j=1,\dots,k$ in MLR are all zeros. $\blacktriangledown$SSE (Sum of Squared Errors) $\text{SSE}=\sum^n_{i=1}\text{res}_i^2=\sum^n_{i=1}(Y_i-\hat{Y}_i)^2$ $\blacktriangledown$deviance = SSE in MLR, which measures the goodness of fit for MLR (smaller == better) $\blacktriangledown$compare SSE of reduced and full models (deviance of reduced and full models) $\blacktriangledown$$d=$ # of coefs in full model - # of coefs in reduced model $\blacktriangledown$$\text{TS}=\frac{(\text{deviance}_{\text{reduced}}-\text{deviance}_{\text{full}})/d}{\hat{\sigma}^2_{\text{full}}}$ $\blacktriangledown$p-value=$P(F>TS)$, where $F\sim F_{d,n-k-1}$. $\blacktriangledown$If p-value < $\alpha\implies$ reject $H_0$. $\blacktriangledown$two special cases: full model F-test in `summary()` and single variable F-test (which is equal to t-test). **6.4 CIs and PIs** $\blacktriangledown$$(1-\alpha)$ CI for $\beta_j$ is  $\hat{\beta}_j\mp t_{n-k-1,\alpha/2}\times SE(\hat{\beta}_j)$. $\blacktriangledown$$(1-\alpha)$ CI for mean of response $\mu\{Y\mid X_1=x_{1,0},\cdots,X_k=x_{k,0}\}$ is $(\hat{\beta}_0+\hat{\beta}_1x_{1,0}+\cdots+\hat{\beta}_k x_{k,0})\mp t_{n-k-1,\alpha/2}\times SE(\hat{\beta}_0+\hat{\beta}_1x_{1,0}+\cdots+\hat{\beta}_k x_{k,0})$ $\blacktriangledown$$(1-\alpha)$ PI for $Y_{new}$ at $(X_{1,new},\cdots,X_{k,new})$ is $(\hat{\beta}_0+\hat{\beta}_1x_{1,new}+\cdots+\hat{\beta}_k x_{k,new})\mp t_{n-k-1,\alpha/2}\times SE\{(\hat{\beta}_0+\hat{\beta}_1x_{1,new}+\cdots+\hat{\beta}_k x_{k,new})-Y_{new}\}$

**7. Model Diagnostics for Linear Regression II** **7.1 R-Squared and Adjusted R-Squared** $\blacktriangledown$Sample variance of the residuals measures the variation in the residuals, $s_{\text{res}}^2=\frac1{n-1}\text{SSE}$ $\blacktriangledown$also mean of residuals is $0$. So SSE also measures the variation in the residuals. $\blacktriangledown$SST (Total Sum of Squares): Due to the existence of  the variation in response. We can use sample variance of the response values to measure it. $\blacktriangledown$ $s^2_Y=\frac1{n-1}\sum^n_{i=1}(Y_i-\bar{Y})^2,\text{ where } \bar{Y}=\frac1n\sum^n_{i=1}Y_i. \text{SST}=\sum^n_{i=1}(Y_i-\bar{Y})^2.$ $\blacktriangledown$SSR (Sum of Squares due to Regression): the variation in the fitted values. $\blacktriangledown$sample variance of the fitted values $s^2_Y=\frac1{n-1}\sum^n_{i=1}(\hat{Y}_i-\bar{Y})^2.$ $\blacktriangledown$$\text{SSR}=\sum^n_{i=1}(\hat{Y}_i-\bar{Y})^2.$ $\blacktriangledown$partitioning variability: $\text{SST}=\text{SSR}+\text{SSE}$, SSR is explained by the regression model while SSE remains unexplained. $\blacktriangledown$R-squared is the percentage of the total response variation explained by the regression model: $R^2=\frac{\text{SSR}}{\text{SST}}=1-\frac{\text{SSE}}{\text{SST}}.$ $\blacktriangledown$If we increase the number of explanatory variables, SSE will decrease but SST is unchanged, then $R^2$ will increase. $\blacktriangledown$Attention for overfitting. $\blacktriangledown$Adjusted R-Squared $\text{Adjusted-}R^2=1-\frac{\text{SSE}/(n-k-1)}{\text{SST}/(n-1)}$ where $n-k-1$ is df of SSE, $n-1$ is df of SST.$\blacktriangledown$ **If we add more explanatory variables in the model, adjusted $R^2$ may not necessarily increase, or may decrease.** $\blacktriangledown$If an addtional variable leads to decrease in adjusted $R^2$, saying it has no prediction power. **7.2 Graphical Tools for Model Diagnostics** **7.2.1 Leverage plot** $\blacktriangledown$It is a measure of the distance between its explanatory variable values and the average of the explanatory variable values in the entire data set. So that it detects the observation with distant explanatory variable values. $\blacktriangledown$"rule of thumb" cut-off value for leverage is $\frac{2(k+1)}{n}$, twice the average of all the leverages. If beyond this value, we call observation $i$ an observation with distant explanatory variable values. **7.2.2 Standardized (Studentized) residuals vs fitted values** $\blacktriangledown$If studentized residual of $i$-th observation falls into the two tails of the $N(0,1)$ distribution, i.e. $|studres|_i$ is too large, greater than $1.96$ or $2$, then we believe it is an outlier. **7.2.3 Cook's distance plot** $\blacktriangledown$$D_i=\sum^n_{j=1}\frac{(\hat{Y}_{j(-1)}-\hat{Y}_j)^2}{(k+1)\hat{\sigma}^2}$ where $\hat{Y}_{j(-i)}$ is the $j$-th fitted value in a MLR fit using all observations except $i$-th observation. $\blacktriangledown$Cook’s distance measures how much removing observations $i$ alters the fitted model. $\blacktriangledown$We call observation with large Cook’s distance an influential observation. $\blacktriangledown$Least squares method is sensitive to influential observations. $\blacktriangledown$**Solution**: examine data for influential points and potentially exclude these observations. Often these observations can provide important information. $\blacktriangledown$Alternative expression of Cook’s distance: $D_i=\frac1{k+1}(\text{studres}_i)^2\frac{h_i}{1-h_i}.$ $\blacktriangledown$Both outliers and distant explanatory variable values could be responsible for large Cook’s distance. (But not necessarily!) $\blacktriangledown$"rule of thumb" cut-off is 1. $\blacktriangledown$Another option is relative comparison. **7.3 Weighted Regression** $\blacktriangledown$Given observations $(X_{1,1},\cdots, X_{k,1}, Y_1),\cdots,(X_{1,n},\cdots,X_{k,n},Y_n)$ a non-constant variance MLR model has the form: $\mu\{Y_i\mid X_{1,i},\cdots,X_{k,i}\}=\beta_0+\beta_1 X_{1,i}+\cdots+\beta_k X_{k,i}, \sigma\{Y_i\mid X_{1,i},\cdots,X_{k,i}\}=\sigma_i,\forall i=1,\cdots,n.$ $\blacktriangledown$Generalized Least Squares (GLS) minimizes $Q(b_0,\cdots,b_k)=\sum^n_{i=1}w_i\{Y_i-(b_0+b_1X_{1,i}+\cdots+b_kX_{k,i})\}^2,$ where $w_i=\frac1{\sigma^2_i}$. $\blacktriangledown$The solution of the estimates in matrix notation is $(\hat{\beta}_{0,\text{GLS}},\hat{\beta}_{1,\text{GLS}},\cdots,\hat{\beta}_{k,\text{GLS}})^T=(\textbf{X}^T\textbf{W}\textbf{X})^{-1}\textbf{X}^T\textbf{W}\textbf{Y},$ where $\textbf{X}$ is the $n\times (k+1)$ design matrix, and $\textbf{W}$ is a matrix with $(w_1,w_2,\cdots,w_{n-1},w_n)$ as diaganol elements, other elements are $0$s. $\blacktriangledown$How to we know the weight matrix $\textbf{W}$ in practice? We use **Feasible Generalized Least Squares (FGLS)** by taking the (ordinary) least squares (OLS) estimation to fit the data first, obtain $\text{res}_i=Y_i-\hat{Y}_i.$ Then let $w_i=\frac1{\text{res}_i^2}.$ $\blacktriangledown$If the constant variance assumption (homoscadasticity) is violated, standard errors for the OLS estimates inaccurately measure uncertainty.

**8. Variable Selection** **8.1 Motivation** $\blacktriangledown$Reason 1: simple models with less variables are preferable. $\blacktriangledown$Reason 2: unnecassary variables, loss of precision, overfitting. **8.2 Sequential Variable Selection** $\blacktriangledown$backward elemination and forward selection $\blacktriangledown$Criteria involved in model selection usually depend on statistical measures. $\blacktriangledown$F-stats "rule of thumb" cut-off is $4$. $\blacktriangledown$F-stat > 4, p-value < 0.05, reject $H_0$, full model is preferred. $\blacktriangledown$F-stat < 4, p-value > 0.05, not reject $H_0$, reduced model is preferred. $\blacktriangledown$Stepwise selection: do one forward selection and one backward elimination step repeatedly, until no explanatory variables can be added or removed. $\blacktriangledown$The parameter `f.out=` in `mle.stepwise()` is the cut-off to remove variables. Similarly, `f.in=` is the cut-off to include variables. $\blacktriangledown$Other statistical measures: SSE needs to be smaller, but the goal for variable selection is to find a small number of explanatory variable if possible. These two contradict, since more explanatory variables means smaller SSE. We need a way to compromise. $\blacktriangledown$Adjusted-$R^2$ can be used as such a statistical measure. $\blacktriangledown$**AIC (Akaike Information Criterion)** and **BIC (Bayesian)** can be considered. $\mu\{Y\mid X_1,\cdots, X_j\}=\beta_0+\beta_1X_1+\cdots+\beta_jX_j, \text{AIC}=n\{\log\left(\frac{\text{SSE}}{n}\right)+1+\log(2\pi)\}+2\times(j+1), \text{BIC}=n\{\log\left(\frac{\text{SSE}}{n}\right)+1+\log(2\pi)\}+\log(n)\times(j+1)$ $\blacktriangledown$For AIC and BIC, if $j$ is the same, then the model with smaller SSE or smaller AIC/BIC is preferred. $\blacktriangledown$Compared to AIC, BIC assigns a larger weight to the number of explanatory variables $j$ in its expression (usually sample size $n$ is large such that $\log(n)>2$). Hence BIC usually prefers the model with less explanatory variables compared to AIC. $\blacktriangledown$So the measures we are looking at are: $-1\times\text{Adjusted-}R^2$, AIC and BIC. **8.3 Variable Selection Among All Subsets** $\blacktriangledown$The variable selection among all subsets is a search through all possible subsets of variables, in order to obtain the resulting mode with the smallest "measure", which is an alternative method for variable selection and is different from the sequential variable selection techniques. $\blacktriangledown$The sequential techniques is a sequential search by either adding or removing a single explanatory variable from the current candidate model at each step. $\blacktriangledown$New statistical measure used in **among all subsets**: $C_p$-statistic. $C_p=(j+1)+(n-j-1)\frac{\text{SSE}/(n-j-1)-\hat{\sigma}^2_{\text{all}}}{\hat{\sigma}^2_{\text{all}}}=\frac{\text{SSE}}{\hat{\sigma}^2_\text{all}}+2(j+1)-n$ $\blacktriangledown$If $j$ is the same, smaller SSE leads to smaller $C_p$, preferred. $\blacktriangledown$If SSE is the same, smaller $j$ leads to smaller $C_p$, preferred. $\blacktriangledown$$C_p$ also compromises how well the model fits the data (SSE) and the number of explanatory variables ($j$) like its previous counterparts AIC, BIC, Adjusted-$R^2$ did. **8.4 Cross Validation for Variable Selection Results** $\blacktriangledown$traning set, testing set. $\blacktriangledown$A measure of predictive ability is mean squared prediction error (MSPE) $\text{MSPE}=\frac1{n_{\text{test}}}\sum^{n_\text{test}}_{l=1}(Y_l-\hat{Y}_l)^2$ $\blacktriangledown$The best model is the model with the smallest MSPE. **8.5 Multicolinearity** $\blacktriangledown$Multicolinearity: one of the explanatory variable $X_j$ can be written as a linear combination of other explanatory variables. $\blacktriangledown$consequence 1: design matrix may not exist, so LS estimates may not be obtained. $\blacktriangledown$consequence 2: even if sometimes $(\textbf{X}^T\textbf{X})^{-1}$, LS are highly unstable and imprecise, SSE of the estimators are large, so hypothesis testing results are not significant. $\blacktriangledown$Variance Inflation Factors (VIF) is a measure of the multicolinearity.$\text{VIF}_j=\frac1{1-R_j^2}$ where $R_j^2$ is the R-squared by regressing $X_j$ on $X_1,\dots,X_{j-1},X_{j+1},\dots,X_k$. $\blacktriangledown$"rule of thumb" cut-off is $10$. $\blacktriangledown$If one explanatory variable $X_j$ with $\text{VIF}_j>10$ should be eliminated. $\blacktriangledown$If multiple explanatory variables have VIFs greater than $10$, then the resulting model is the one after dropping the explanatory variable **has the best fitting (smallest SSE or deviance)**.

**9. Logistic Regression for Two-Category Response Variables and Its Estimation** **9.1 Two-Category Response Variables** $\blacktriangledown$"Either this, or that." **9.2 Motivating Example** **9.3 Binary Logistic Regression Model** $\blacktriangledown$A generalised linear model (GLM) is a model where the mean of the response is related to the explanatory variables via the following relationship:$g(\mu\{Y\mid X_1,\dots,X_k\})=\beta_0+\beta_1X_1+\cdots+\beta_kX_k.$ $\blacktriangledown$$g(\cdot)$ is called the link function, which depends on the type of the response variable. $\blacktriangledown$We call the model with a specific link for two-category response: **binary logistic regression** model. $\blacktriangledown$Binary logistic regression model assumptions: $\blacktriangledown$ 1. **Bernoulli distribution**: there is a Bernoulli distributed (sub)population of responses for given values of the explanatory variables. $\blacktriangledown$ 2. **Generalized linearity**: the transformation of the mean of the response falls on a liner function of the explanatory variables $g(\mu\{Y\mid X\}=\beta_0+\beta_1X_1+\cdots+\beta_kX_k,\forall X=(X_1,\cdots,X_k),\text{ where } g(u)=\log\left(\frac{u}{1-u}\right)$ which is called logic link function. The inverse of logit link function is $g^{-1}(v)=\frac{e^v}{1+e^v}\in[0,1].$ Then $\mu\{Y\mid X\}=g^{-1}(\beta_0+\beta_1X_1+\cdots+\beta_kX_k)\in[0,1].$ $\blacktriangledown$ 3. **Independence:** ... $\blacktriangledown$ Interpretation$P(Y=1\mid X)=\mu\{Y\mid X\}=g^{-1}(\beta_0+\beta_1X_1+\cdots+\beta_kX_k)=\frac{e^{\beta_0+\beta_1X_1+\cdots+\beta_kX_k}}{1+e^{\beta_0+\beta_1X_1+\cdots+\beta_kX_k}},\frac{P(Y=1\mid X)}{1-P(Y=1\mid X)}=e^{\beta_0+\beta_1X_1+\cdots+\beta_kX_k}$ which is called **odds that $Y=1\mid X$.** **9.4 Estimation of Binary Logistic Regression** $\blacktriangledown$Likelihood function $\mathcal{L}=P(Y_1=y_1,\cdots, Y_n=y_n\mid\text{given all } Xs)=\prod^n_{i=1}\{p_i(\beta_0,\cdots,\beta_k)\}^{y_i}\{1-p_i(\beta_0,\cdots,\beta_k)\}^{1-y_i}$ $\blacktriangledown$We choose MLE $\hat{\beta}_0,\cdots, \hat{\beta}_k$ numerically to maximize $\mathcal{L}$. No closed form formula for these estimators. $\blacktriangledown$The fitted probabilities are given by $\hat{\pi}(X)=\hat{\mu}(Y\mid X)=g^{-1}(\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k)=\frac{\exp{(\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k)}}{1+\exp{(\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k)}}$ **9.5 Prediction of a New Observation** $\blacktriangledown$The forecast of probabilities is given by $\hat{\pi}(X_{\text{new}})=\hat{\mu}(Y\mid X_{\text{new}})=g^{-1}(\hat{\beta}_0+\hat{\beta}_1X_{1,\text{new}}+\cdots+\hat{\beta}_kX_{k,\text{new}})=\frac{\exp{(\hat{\beta}_0+\hat{\beta}_1X_{1,\text{new}}+\cdots+\hat{\beta}_kX_{k,\text{new}})}}{1+\exp{(\hat{\beta}_0+\hat{\beta}_1X_{1,\text{new}}+\cdots+\hat{\beta}_kX_{k,\text{new}})}}$ $\blacktriangledown$commonly used threshold for predicting the response is $\pi(X)=0.5$. $\blacktriangledown$$\hat{Y}_\text{new}=1$ if $\hat{\pi}(X_\text{new})>0.5$; $\hat{Y}_\text{new}=0$ otherwise.

**10. Inferential Tools and Variable Selection for GLM** **10.1 MLE for GLM and SE** $\blacktriangledown$Formula for SE not introduced. $\blacktriangledown$The SE formula can only be given when the sample size $n$ is large enough. We call it $\text{SE}_a(\hat{\beta}_j)$. $\blacktriangledown$practical sampling distributions of $\hat{\beta}_j,j=0,\cdots,k$, $\frac{\hat{\beta}_j-\beta_j}{\text{SE}_a(\hat{\beta}_j)}\overset{a}{\sim}N(0,1)$ $\blacktriangledown$When the sample size $n$ is small, we use **bootstrap** to obtain the SE and the practical sampling distribution. **10.2 Hypothesis Testing** **10.2.1 z-Test** $\blacktriangledown$$H_0:\beta_j=0$ vs $H_a:\beta_j\not=0$ $\blacktriangledown$$\text{TS}=\frac{\hat{\beta}_j-0}{\text{SE}_a(\hat{\beta}_j)}$ $\blacktriangledown$p-value=$2\times P(Z>|\text{TS}|)$, where $Z\sim N(0,1)$ **10.2.2. $\chi^2$-Test** $\blacktriangledown$Similar to the F-test in MLR, $\chi^2$-test is used to test whether or not a subgroup of $\beta_j,j=1,\cdots,k$ in GLM are all zeros. $\blacktriangledown$The **deviance** for a GLM $g(\mu\{Y\mid X\})=\beta_0+\beta_1X_1+\cdots+\beta_kX_k$, where $X=(X_1,\cdots,X_k)$ is defined by $\text{deviance}=-2\log\mathcal{L}(\hat{\beta}_0,\cdots,\hat{\beta}_k)$ $\blacktriangledown$Deviance measures the goodness of fit for GLM. $\blacktriangledown$The smaller the better. $\blacktriangledown$Drop in deviance is defined by $\text{deviance}_{\text{reduced}}-\text{deviance}_{\text{full}}$. The drop-in-deviance provides an indication of the importance of the variables that have been excluded from the full model. $\blacktriangledown$$\text{TS}=\text{deviance}_{\text{reduced}}-\text{deviance}_{\text{full}}$ compared with $\chi_d^2$ distribution, where $d$ is the number of $\beta$s being tested, $d=$ # of coefs in full - # of coefs in reduced. $\blacktriangledown$two special cases of $\chi^2$-test, full and single. **10.3 CIs** $\blacktriangledown$$(1-\alpha)$ CI for $\beta_j$ is $\hat{\beta}_j\mp z_{\alpha/2}\times \text{SE}_a(\hat{\beta}_j)$ where $Z_{0.05/2}=1.96$. **10.4 Variable Selection** $\blacktriangledown$Two contradicting goals: smaller deviance and smaller number of explanatory variables. $\blacktriangledown$again, AIC and BIC $g(\mu\{Y\mid X_1,\cdots,X_j\})=\beta_0+\beta_1X_1+\cdots+\beta_jX_j,\text{AIC}=\text{deviance} + 2\times(j+1),\text{BIC}=\text{deviance}+\log(n)\times(j+1)$ $\blacktriangledown$again BIC prefers the model with less explanatory variables. $\blacktriangledown$forward selection, backward elimination, stepwise (1+1).

**11. Multicategory Response Regression** **11.1 Multicategory Response Variables** $\blacktriangledown$ordinal is a special case of nominal, also the ordinal has more information. **11.2 Nominal Response Regression Models** $\blacktriangledown$change the categories from $c=2$ in binary logistic regresssion to $c=C$, then the nominal response regression model (baseline-category logit model) is $\frac{\pi_c}{\pi_1}=\exp{(\beta_{c0}+\beta_{c1}X_1+\cdots+\beta_{ck}X_k)}, \textbf{ only for } c=2,\cdots,C.$ **11.3 Ordinal Response Regression Models** $\blacktriangledown$sps categories $c=1,\cdots,C$ and category 1 < category 2 < $\cdots$ < category $C$. $P(Y\leq c)=\pi_1+\cdots+\pi_C \forall c=1,\cdots,C$ $\blacktriangledown$The ordinal response regression model is $\text{odds that } Y\leq c=\frac{P(Y\leq c)}{1-P(Y\leq c)}=\frac{\pi_1+\cdots+ \pi_c}{\pi_{c+1}+\cdots+\pi_C}=\exp{(\beta_{c0}+\beta_{c1}X_1+\cdots+\beta_{ck}X_k)}\textbf{ only for } c=1,\cdots,C-1.$ $\blacktriangledown$Note that $P(Y\leq C)\equiv1.$

**12. Logistic Regression for Binomial Counts** **12.1 Motivating Example** **12.2 Logistic Regression for Binomial Counts** $\blacktriangledown$ In a typical aggregated dataset, explanatory $X$, with binomial count $Z$ and total number $M$. $Z$ is the number of "success" given some specific $X$, $M$ is the number of "trials" given some specific $X$. Given $X$, the probability of one "success" in a trial is denoted by $\pi\in[0,1]$. $\blacktriangledown$ Given $X$, the proportion of success is $\frac{Z}{M}$, which can be used to estimate $\pi$ when $M$ is large. $\blacktriangledown$ **Assumption 1** of binomial logistic regression model: there is a binomial distributed (sub)population of responses $Z$ for given values of the explanatory variables. $\blacktriangledown$ **Assumption 2** of binomial logistic regression model: the transformation of the probability of "success" $\pi$ falls on a linear function of the explanatory variables $g(\pi)=\beta_0+\beta_1X_1+\cdots+\beta_kX_k,\forall X=(X_1,\cdots,X_k),\text{ where } g(u)=\log\left(\frac{u}{1-u}\right).$ $\blacktriangledown$ **Assumption 3**: independence. $\blacktriangledown$ 95\% CI for $\beta_k$: $\hat{\beta}_k\mp 1.96\text{SE}(\hat{\beta}_k)$ $\blacktriangledown$Estimated (fitted) probability of "success" is $\hat{\pi}=g^{-1}(\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k)=\frac{\exp{(\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k)}}{1+\exp{(\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k)}},g(\hat{\pi})=\log(\frac{\hat{\pi}}{1-\hat{\pi}})=\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k$ $\blacktriangledown$ $\textbf{predicted: }\hat{\pi}=g^{-1}(\hat{\beta}_0+\hat{\beta}_1X_{1,\text{new}}+\cdots+\hat{\beta}_kX_{k,\text{new}})=\frac{\exp{(\hat{\beta}_0+\hat{\beta}_1X_{1,\text{new}}+\cdots+\hat{\beta}_kX_{k,\text{new}})}}{1+\exp{(\hat{\beta}_0+\hat{\beta}_1X_{1,\text{new}}+\cdots+\hat{\beta}_kX_{k,\text{new}})}}$ $\blacktriangledown$That was **fitted probability** above, we are really into **fitted reponse** in the end. The mean of response is $\mu\{Z\mid X\}=M\pi$, the fitted values of response $Z$ are $\hat{Z}=\hat{\mu}\{Z\mid X\}=M\hat{\pi}.$ **12.3 Model Diagnostics** **12.3.1 Logit proportion vs explanatory variable plot** $\blacktriangledown$plot the logit proportion $g(\frac{Z}{M})$ vs explanatory variables $\blacktriangledown$should be a straight line, otherwise the model assumption is violated $\blacktriangledown$since $g(\frac{Z}{M})$ is undefined for $\frac{Z}{M}$ equal to $0$ or $1$, we need to add some variations to these values while plotting. **12.3.2 Pearson residual plot** $\blacktriangledown$The residual for binomial count $Z$ defined as $\text{res}_i=Z_i-\hat{Z}_i$ for obs. $i$. $\blacktriangledown$ $\text{SD}(\text{res}_i)=\sqrt{M_i\pi_i(1-\pi_i)}$ $\blacktriangledown$$\text{SE}(\text{res}_i)=\sqrt{M_i\hat{\pi}_i(1-\hat{\pi}_i)},\text{ where } \hat{\pi}_i=\frac{\exp{(\hat{\beta}_0+\hat{\beta}_1X_{1,i}+\cdots+\hat{\beta}_kX_{k,i})}}{1+\exp{(\hat{\beta}_0+\hat{\beta}_1X_{1,i}+\cdots+\hat{\beta}_kX_{k,i})}}$ $\blacktriangledown$$\text{Peares}_i=\frac{\text{res}_i}{\text{SE}({\text{res}_i})}$ $\blacktriangledown$Using **Pearson residuals** allows the residuals to be viewd on the same scale. $\blacktriangledown$For large values of $M_i\ (>5)$, the Pearson residuals are roughtly standard normally distributed, if other assumptions are satisfied. $\blacktriangledown$As standard normally distributed, the cut-off for $|\text{Peares}_i|$ is $1.96$ or $2$. Beyond this value, outlier. **12.3.3 Deviance goodness-of-fit test** $\blacktriangledown$This is weried but the test seems to have "reversed" null and alt. hypotheses: $\blacktriangledown$$H_0$: Binomial model $g(\pi)$ is appropriate. $\blacktriangledown$$H_a$: not appropriate.

**13. Log-Linear Regression for Poisson Counts** **13.1 Motivating Examples** $\blacktriangledown$"Rare events": the probability $\pi$ of an event is small. $\blacktriangledown$When number of total trials $M$ is large and $\pi$ is small, we have this approximation: $P(Z=z)={M\choose z}\pi^z(1-\pi)^{M-z}\approx\frac{e^{-\mu}\mu^z}{z(z-1)(z-2)\cdots 1}$ $\blacktriangledown$$M$ is gone. $\blacktriangledown$Poisson count: In real data, we only know the number of successes $Z$, but no number of total trials $M$, and $M$ is large, while the probability of success is small, then we call $Z$ a **Poisson count**. **13.2 Log-Linear Regression for Poisson Counts** $\blacktriangledown$**Assumption 1 (Poisson distribution)**: There is a Poisson distributed (sub)population of responses $Z$ for given values of the explanatory variables. $\blacktriangledown$**Assumption 2 (Generalized linearity)**: $g(\mu\{Z\mid X\}=\beta_0+\beta_1X_1+\cdots+\beta_kX_k,\forall X=(X_1,\cdots,X_k),\text{ where } g(u)=\log(u).$ $\blacktriangledown$**Assumption 3 (Independence)**. $\blacktriangledown$As when $\beta_i$ is small $e^{\beta_i}\approx 1+\beta_i$, so $\mu\{Z\mid X_1=x_1+1,X_2,\cdots,X_k\}\approx (1+\beta_1)\mu\{Z\mid X_1=x_1,\cdots,X_k\}.$ $\blacktriangledown$$\beta_1$ is the percentage increase in the mean of response $Z$ for one unit increasem in $X_1$. $\blacktriangledown$CIs $\blacktriangledown$Drop-in-deviance $\chi^2$-test $\blacktriangledown$fitted values of response $Z$: $\hat{Z}=\hat{\mu}\{Z\mid X\}=e^{\hat{\beta}_0+\hat{\beta}_1X_1+\cdots+\hat{\beta}_kX_k}$ $\blacktriangledown$ prediction **13.3 Model Diagnostics** **13.3.1 Log response vs explanatroy variable plot** $\blacktriangledown$should be a straight line with no violations $\blacktriangledown$$\log(Z)$ is undefined for $Z=0$, jittering. **13.3.2 Pearson residual plot** $\blacktriangledown$Similarly as before, a Pearson residual is a residual divided by its standard error. If the observation is from the Poisson log-linear model with the all the assumptions satisfied, Pearson residuals should be roughly standard normally distributed. $\blacktriangledown$cut-off $\pm 1.96$ or $\pm 2$. **13.3.3 Deviance goodness-of-fit test** $\blacktriangledown$$H_0$: Poisson model $g(\mu)$ is appropriate. $\blacktriangledown$$H_a$: not appropriate

**14. Bootstrap** **14.1 Simulation** $\blacktriangledown$benefits of simulation: no need of knowing the formula for sampling distribution, provide insights of statistical concepts **14.2 Bootstrap** $\blacktriangledown$The **bootstrap** is a computationally intensive method based on the idea of randomly drawing **bootstrap samples** with **replacement** from $\{Z_i,X_i\}^m_{i=1}$. $\blacktriangledown$ R repeated bootstrap samples (bootsrap datasets) $\{Z_i^{*(1)},X_i^{*(1)}\}^m_{i=1},\cdots,\{Z_i^{*(R)},X_i^{*(R)}\}^m_{i=1}$ **14.2.1 Boostrap standard errors** $\blacktriangledown$The **bootstrap standard error of $\hat{\beta}_i$** is denoted by $\text{SE}_b(\hat{\beta}_i)$, which provides a good estimate of $\text{SD}(\hat{\beta}_i)$ when sample size $m$ is small. $\blacktriangledown$Comparison between $\text{SE}_a(\hat{\beta}_1)$ and $\text{SE}_b(\hat{\beta}_1)$ $\blacktriangledown$$\text{SE}_a(\hat{\beta}_1)$ is a good estimate od $\text{SD}(\hat{\beta}_1)$ when the sample size $m$ is large enough. $\blacktriangledown$$\text{SE}_b(\hat{\beta}_1)$ is good when $m$ is either large or small. $\blacktriangledown$Benefits of boostrap I: estimate sampling distributions of parameter estimation, estimate the standard deviation of the estimation no need of knowing the formulas for sampling distribution and standard deviation/standard error, sample size $m$ either large or small, repetition is required, computationally intensive **14.2.2 Boostrap CIs** $\blacktriangledown$The distribution of $\hat{\beta}_1-\beta_1$ can be well approx via $R=1000$ different $\hat{\beta}_1^{*(1)}-\hat{\beta}_1,\hat{\beta}_1^{*(2)}-\hat{\beta}_1,\cdots,\hat{\beta}_1^{*(R)}-\hat{\beta}_1$, no matter $m$ is small or large. $\blacktriangledown$Based on the quantiles, which are determined by the distribution, we can find $\alpha/2$ quantile and $1-\alpha/2$ quantile. Then the CI is trivial. We can this **Efron's Bootstrap Percentile CI**. $\blacktriangledown$**Efron's Bootstrap Percentile CI for mean of response** is based on the distribution of $\hat{\mu}^{*(1)}-\hat{\mu},\hat{\mu}^{*(2)}-\hat{\mu},\cdots,\hat{\mu}^{*(R)}-\hat{\mu}.$ $\blacktriangledown$Bootstrap CI Idea: CI of $e^{\beta_1}$, estimation $e^{\hat{\beta}_1}$, $P(L\leq e^{\hat{\beta}_1}-e^{\beta_1} \leq U)=1-\alpha$, distribution of $e^{\hat{\beta}_1}-e^{\beta_1}$ ($m$ is small), find quantiles, CI is $[e^{\hat{\beta}_1}-U,e^{\hat{\beta}_1}-L].$ $\blacktriangledown$Benefits of bootstrap II: $m$ is either large or small, bootstrap CI can be used, while classical CI can only be used when $m$ is large.

<!-- # 15. PCA (not in final) -->

<!-- ## 15.1 Motivating Example -->
<!-- - multivariate data -->

<!-- ## 15.2 Linear Combination and PCA -->
<!-- - reduce length of such vector that account for most of the information in the original dataset. -->
<!-- - mean is ok, but equally weighted -->
<!-- - pca seeks the linear combination of the original variables which contains the maximal variance (variation) -->
<!-- - requirements -->
<!-- - loadings -->

<!-- ## 15.3 PCA Usage -->
<!-- - when number of variables is large, use pca to avoid multicolinearity -->

<!-- ## 15.4 When is it appropriate to use PCA? -->
<!-- - for a large numberof correlated variables, reduce to a small set that still contains most of variation info -->
<!-- - pcs are uncorrelated. -->
<!-- - disadvantage: pcs are difficult to interpret, cannot provide better prediction in regression. -->

|                            | Continuous $X$ + Categorical $X$                 |
|:--------------------------:|:------------------------------------------------:|
|Continuous $Y$              | MLR + Indicator Variables                        |
|Two-Category $Y$            | Binary Logistic Regression + Indicator Variables |
|Multicategory $Y$ - Nominal | Nominal Response Regression + Indicator Variables|
|Multicategory $Y$ - Ordinal | Ordinal Response Regression + Indicator Variables|
|Binomial Count $Z$          | Binomial Logistic Regression + Indicator Variables|
|Poisson Count $Z$           | Poisson Regression + Indicator Variables         |

![](./Figs/qq.png)
