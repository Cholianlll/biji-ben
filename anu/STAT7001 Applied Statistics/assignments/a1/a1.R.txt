library(Sleuth3)
library(wle)

# STAT7001 Assignment 1
# 2017-09-17

# Question 1
data <- ex1225
head(data)
attach(data)
mlr <- lm(log(WeeklyEarnings)~Age+factor(Sex)+factor(MaritalStatus)+EdCode)

# (a)
# The least square estimate for the coefficient of "EdCode" is 0.1121. It means when holding other independent variables constant, if we increase "EdCode" by 1, the dependent variable, i.e. the logarithm of WeeklyEarnings will increase by 0.1121.

summary(mlr)

# (b)
# Null hypothesis: The estimated coefficients ($\beta_0,\beta_{\text{Age}}, \beta_{\text{Sex}},\beta_{\text{MaritalStatus}},\beta_{\text{EdCode}})$ are all $0$s.
# Alternative hypothesis: At least one of the estimated coefficients above is
# not $0$.
# Since p-value is $2.2\times 10^{-16}<0.05$, so reject the null hypothesis.


# (c)
lm.full <- mlr
lm.null <- lm(log(WeeklyEarnings)~1)
drop1(lm.full,test="F")

# or
Y <- log(WeeklyEarnings)
X <- cbind(Age,Sex,MaritalStatus,EdCode)
mle.stepwise(Y~X,type="Backward")

# or 

mle.stepwise(log(WeeklyEarnings)~Age+factor(Sex)+factor(MaritalStatus)+EdCode,
             type="Backward")
mle.stepwise(log(WeeklyEarnings)~Age+factor(Sex)+factor(MaritalStatus)+EdCode,
             type="Forward")

# In fact, we did not drop any term in our fitted model. So we keep Age, Sex, MaritalStatus and EdCode to predict the logarithm of “WeeklyEarnings” by via backward elimination.

# (d) R codes


# Question 2

# (a)
summary(mlr)
# R-squared, only 26.82% of variation can be explained by the model. 

# (b)
plot(mlr, which=1, pch=16, cex=0.6)
# It seems that our residuals vs. fitted plot violates the assumption of homoscedasticity as some data in the middle quantile have relatively small residuals.

# (c)
plot(mlr, which=2, pch=16, cex=0.6)
# Clearly, the lower quantile data deviates from the line in Q-Q plot, thus our assumption of normality is violated, i.e. the data is not normally distributed.

# (d)
plot(mlr, which=4, pch=16, cex=0.6)
# The "rule of thumb" cut-off for Cook's distance is 1, our largest Cook's distance is still less than $0.020$. Also, it is not relatively larger than others. So we claim that there are no influential observations here.

# (e)
which.max(cooks.distance(mlr))
# The observation 6242 has the largest Cook's distance.

rstudent(mlr)[6242]
# Since the studentized residual of observation 6242 is $-14.09 < -2$, so we believe it is an outlier. We usually delete the observation from the original dataset and refit the model.

# (f)
lev <- hat(cbind(Age, factor(Sex), factor(MaritalStatus), EdCode))
lev[6242]
# The leverage of observation 6242 is 0.0004889895
(lev.cutoff <- 2*(4+1)/nrow(data))
# So the leverage of observation 6242 is less than the "rule of thumb" cut-off therefore, it does not have distant explanatory variable values.

# (g) R codes

# Question 3

# (a)
levels(Region)
levels(MetropolitanStatus)

IMidwest=ifelse(Region=="Midwest",1,0) 
INortheast=ifelse(Region=="Northeast",1,0) 
ISouth=ifelse(Region=="South",1,0)

IMetropolitan=ifelse(MetropolitanStatus=="Metropolitan",1,0) 
INotMetropolitan=ifelse(MetropolitanStatus=="Not Metropolitan",1,0)

# Should use "Private" as the baseline level for the categorical variable "JobClass", since we are interested in if the "difference between private and government jobs" are significant, and all of the three "FedGov", "StateGov" and "LocalGov" are government jobs. Hence, we set the category "Private" as the baseline to construct detailed comparisons.

levels(JobClass)
IFedGov <- ifelse(JobClass=="FedGov",1,0)
ILocalGov <- ifelse(JobClass=="LocalGov",1,0)
IStateGov <- ifelse(JobClass=="StateGov",1,0)

# Consequently, we select "FedGov", "LocalGov" and "StateGov" as indicator variables.


# (b)
mlr2 <- lm(log(WeeklyEarnings)~Age+factor(Sex)+factor(MaritalStatus)+EdCode+
             IMidwest+INortheast+ISouth+
             IMetropolitan+INotMetropolitan+
             IFedGov+ILocalGov+IStateGov)

summary(mlr2)

# The p-value of "IFedGov" is less than 0.05, so it is significantly different from the category of "Private", but the p-values of "ILocalGov" and "IStateGove" are greater than 0.05 so that these two categories are not significantly different from "Private" category.

# (c)
mlr2.reduce <- lm(log(WeeklyEarnings)~Age+factor(Sex)+factor(MaritalStatus)+EdCode+
                    IMidwest+INortheast+ISouth+
                    IMetropolitan+INotMetropolitan)
anova(mlr2.reduce, mlr2, test="F")

# F-statistic is 18.86 while p-value is less than 0.05. As a result, we suggest that we should reject null hypothesis and at least one category has a different level of the mean of log(WeeklyEarnings) compared to the category of "Private".

# (d)
anova(mlr2)
mlr2.inter <- lm(log(WeeklyEarnings)~Age+factor(Sex)+factor(MaritalStatus)+EdCode+
                   IMidwest+INortheast+ISouth+
                   IMetropolitan+INotMetropolitan+
                   IFedGov+ILocalGov+IStateGov+factor(Sex)*factor(MaritalStatus))
anova(mlr2.inter)

# The fitted model from Q3b has $\text{SSE}=3048.87$, while the model with an extra interaction term has $\text{SSE}=3043.89$.

# So the second model has smaller SSE, i.e. less unexplained variation.

# (e)
summary(mlr2.inter)

# Question 4

# (a)
set.seed(7001)
beta0 <- 2
beta1 <- 1
beta2 <- -1
n <- 100
R <- 1000
hatbeta0 <- rep(0,R)
hatbeta1 <- rep(0,R)
hatbeta2 <- rep(0,R)
responses <- rep(0,R)
x0 <- data.frame(X1=2.5,X2=0)
CIs <- NULL
X2 <- rt(n,3)
for (r in 1:R){
  X1 <- 1:n
  errors <- rnorm(n)
  Y <- beta0+beta1*X1+beta2*X2+errors
  sim.mlr <- lm(Y~X1+X2)
  hatbeta0[r] <- sim.mlr$coef[1]
  hatbeta1[r] <- sim.mlr$coef[2]
  hatbeta2[r] <- sim.mlr$coef[3]
  CIs <- rbind(CIs,predict(sim.mlr,x0,interval='confidence',level=0.95))
  responses[r] <- sim.mlr$coef[1] + sim.mlr$coef[2]*2.5 + sim.mlr$coef[3]*0
}

mean(hatbeta0)
mean(hatbeta1)
mean(hatbeta2)

mean(responses) # 4.502998

# (b)
theo.response <- 2+1*2.5+(-1)*0
sum(theo.response > CIs[,2] & theo.response < CIs[,3])

# (c)
# Based on the previous information, if we resample, we could approximately find out that 95% of the intervals would contain the population mean.

