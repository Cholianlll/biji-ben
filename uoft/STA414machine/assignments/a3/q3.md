# STA414 Assignment 3

###Rui Qiu \#999292509

2015-11-29

## 1. Fitting a linear model

The direct summary of simple linear model can be shown as:

```{r}
Call:
  lm(formula = train1y ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8, 
     data = data.frame(train1x))

Residuals:
  Min       1Q   Median       3Q      Max 
-1.40576 -0.31558 -0.03542  0.26756  1.95052 

Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
(Intercept)  5.09323    0.44713  11.391  < 2e-16 ***
  V1           0.21757    0.02145  10.142  < 2e-16 ***
  V2           1.58882    0.71488   2.222  0.02718 *  
  V3           2.56441    0.60984   4.205 3.68e-05 ***
  V4           1.90180    0.44280   4.295 2.53e-05 ***
  V5          -0.65827    0.23777  -2.769  0.00607 ** 
  V6           0.30995    0.29234   1.060  0.29008    
  V7           0.26580    0.05045   5.268 3.05e-07 ***
  V8          -0.46711    0.11209  -4.167 4.30e-05 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5391 on 241 degrees of freedom
Multiple R-squared:  0.594,  Adjusted R-squared:  0.5805 
F-statistic: 44.07 on 8 and 241 DF,  p-value: < 2.2e-16
```

- By observation, the `Adjusted R-squared` value is `0.5805`, not a good sign.
- The `Pr(>|t|)` for `V6` is comparatively high, probabily we can ignore it when fitting a linear model, so the re-do of fitting is below:

```{r}
Call:
  lm(formula = train1y ~ V1 + V2 + V3 + V4 + V5 + V7 + V8, data = data.frame(train1x))

Residuals:
  Min       1Q   Median       3Q      Max 
-1.33637 -0.30870 -0.04837  0.27276  2.00091 

Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
(Intercept)  4.99873    0.43826  11.406  < 2e-16 ***
  V1           0.22210    0.02103  10.563  < 2e-16 ***
  V2           1.60312    0.71494   2.242   0.0258 *  
  V3           2.61755    0.60793   4.306 2.42e-05 ***
  V4           1.98178    0.43644   4.541 8.84e-06 ***
  V5          -0.70383    0.23391  -3.009   0.0029 ** 
  V7           0.27002    0.05031   5.368 1.87e-07 ***
  V8          -0.43856    0.10883  -4.030 7.48e-05 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.5392 on 242 degrees of freedom
Multiple R-squared:  0.5921,  Adjusted R-squared:  0.5803 
F-statistic: 50.18 on 7 and 242 DF,  p-value: < 2.2e-16
```

- Therefore, 

		The MSE in terms of 8 covariates is  0.2801569 
 		The MSE in terms of 7 covariates is  0.2814637 

- And the prediction is `y_predict`.
 		

## 2. Fitting a Gaussian process model with linear covariance

- The calculated MSE of fitting a Gaussian process model with linear covariance is `0.443493`.

## 3. Fitting Gaussian process model without/with rescaling

- Set the `res` and `res2` matrix as two 3 by 400 matrices, considering the step, starting and ending values of hyperparameters.
- The two generated text files, *output1.txt* and *output2.txt* are hyperparameters candidates for non-scaling GP and re-scaling GP respectively. The second line and third line of such files are values of `gamma` and `rho`.
- *output1.txt* and *output2.txt* are included in appendices.
- The minimizer hyperparameters `gamma` and `rho` for MSE are acquired by calling `index` and `index2` in console after running our code, then call `res[,index]` and `res2[,index2]` respectively.

```{r}
> res[,index]
[1] 60.76519  9.60000  0.16000
> res2[,index2]
[1] 68.40608  5.10000  0.96000
```

- For non-scaling, `gamma == 9.6`, `rho == 0.16`
- For rescaling, `gamma == 5.1`, `rho == 0.96`
- The MSE for non-scaling Gaussian process model is `0.2934913`.
- The MSE for rescaling Gaussian process model is `0.2403158`.

## 4. Runtime comparison

- By using `proc.time()` we can get runtime for each model fitting.

		simple linear:
		   user  system elapsed 
		  0.022   0.002   0.025 
		
		
		gaussian process with linear covariance:
		   user  system elapsed 
		 18.144   0.036  18.181 
		
		
		gaussian process non-scaling:
		    user   system  elapsed 
		2425.194    7.831 2434.902 
		
		
		gaussian process scaling:
		    user   system  elapsed 
		2505.971    6.253 2515.121

## 5. Summary

#### Using 5-fold cross-validation

model    | simple linear | GP with linear cov | GP with hyperpar | GP with hyper (rescaling)
--------- | --------- | --------- | --------- | ----
MSE | 0.2801569 | 0.443493 | 0.2934913 | 0.2403158
runtime  | 0.022 | 18.144 | 2425.194 | 2505.971

- Since Gaussian process with hyperparameters (re-scaling) has the smallest MSE among all, it is the best model fitting for our data. Simple linear model has a larger MSE, and Gaussian process with hyperparameters (non-scaling) has an MSE slightly larger than the previous one. In general, all of three MSEs are quite close to each other.
- However, Gaussian process with linear covariance has a rather outlying MSE, probabily because its parameters are not carefully selected to minimize the MSE.
- The runtime of simple linear model and GP with linear covariance are tolerable. But GP with hyperparameters takes much longer time (approx. 40 minutes) to run due to the existence of nested for-loop inside code.

#### Using 10-fold cross-validation

model | GP with hyperpar | GP with hyper (rescaling)
--------- | --------- | ---------
MSE | 0.2935 | 0.2406

- During a previous pilot run, I change the required 5-fold cross-validation to 10-fold, which is widely used in practice. It turns out that the MSE for linear and GP with linear covaraince don't change (of course they don't, they don't have hyperparameters!). What I actually want to say is that the MSE with/without rescaling are only tiny little bit higher than 5-fold. So the number of folds does not have great influence on MSE here.
- And runtime of 5-fold and 10-fold are similar.

## 6. Appendices

- `a3.r`

```{r}
# set up!
train1x <- as.matrix(read.table("./train1x", header=F))
train1y <- as.matrix(read.table("./train1y", header=F))
testx <- as.matrix(read.table("./testx", header=F))
testy <- as.matrix(read.table("./testy", header=F))

# divide!
section <- function(train1x, train1y, i) {
  x_test <- train1x[((i-1)*50+1):(i*50),]
  x_train <- train1x[-(((i-1)*50+1):(i*50)),] 
  y_test <- as.matrix(train1y[((i-1)*50+1):(i*50)])
  y_train <- as.matrix(train1y[-(((i-1)*50+1):(i*50))])
  return(list(x_test,x_train,y_test,y_train))
}

# covariance functions!
K1 <- function(i,j) {
  return(100^2*(i%*%j))
}

K2 <- function(gamma,rho,x,y) {
  return(100^2+gamma^2*exp(-(rho^2)*sum((x-y)^2)))
}

# cross-validation!
cv <- function(gamma, rho, train1x, train1y) {
  MSE <- matrix(0,1,5)
  for (k in 1:5) {
    x_test <- section(train1x,train1y,k)[1][[1]]
    x_train <- section(train1x,train1y,k)[2][[1]]
    y_test <- section(train1x,train1y,k)[3][[1]]
    y_train <- section(train1x,train1y,k)[4][[1]]
    C <- matrix(0,200,200)
    for (i in 1:200) {
      for (j in 1:200) {
        C[i,j] <- K2(gamma,rho,x_train[i,],x_train[j,])
      }
    }
    C <- C + diag(200)
    predict <- matrix(0,1,50)
    for (i in 1:50) {
      t <- matrix(0,1,200)
      for (j in 1:200) {
        t[j] <- K2(gamma,rho,x_train[j,],x_test[i,])
      }
      predict[i] <- t%*%solve(C)%*%y_train
    }
    MSE[k] <- sum((t(y_test) - predict)^2)
  }
  return(sum(MSE))
}
```
- `script.r`

```{r}
source("a3.r")

########## simple linear model ########## 
t1 <- proc.time()

mse <- function(m) { 
  mse <- mean(m$residuals^2)
  return(mse)
}

m1 <- lm(train1y~V1+V2+V3+V4+V5+V6+V7+V8, data=data.frame(train1x))
summary(m1)
mse(m1)

## what if we drop V6?
m2 <- lm(train1y~V1+V2+V3+V4+V5+V7+V8, data=data.frame(train1x))
summary(m2)
mse(m2)

cat(" The MSE in terms of 8 covariates is ", mse(m1), "\n", 
    "The MSE in terms of 7 covariates is ", mse(m2), "\n")

## prediction
y_predict <- predict(m2,newdata=data.frame(testx),interval='prediction')

t2 <- proc.time()
print(t2-t1)

########## Gaussian process with linear covariance ##########
t1 <- proc.time()
C <- matrix(0,250,250)
for (i in 1:250) {
  for (j in 1:250) {
    C[i, j] <- K1(train1x[i,],train1x[j,])
  }
}

C <- C + diag(250)

predict <- matrix(0,1,2500)
for (i in 1:nrow(testy)) {
  t <- matrix(0,1,250)
  for (j in 1:250) {
    t[j] <- K1(train1x[j,],testx[i,])
  }
  predict[i] <- t%*%solve(C,train1y)
}

MSE2 <- sum((t(testy) - predict)^2)/2500
print(MSE2)
t2 <- proc.time()
print(t2-t1)

########## Gaussian process with hyperparameters (non-scaling) ##########
t1 <- proc.time()
res <- matrix(0,3,400)
num <- 1
for (gamma in seq(0.1,10,0.5)) {
  for (rho in seq(0.01,1,0.05)) {
    res[1,num] <- cv(gamma,rho,train1x,train1y)
    res[2,num] <- gamma
    res[3,num] <- rho
    num <- num + 1
    # it's more like a progress tracking feature, not really need this
    cat(num, res[1,num-1],res[2,num-1],res[3,num-1],"\n")
  }
}

write.table(res, "./output1.txt", sep="\t")

index <- which(res[1,] == min(res[1,]))
gamma <- res[2,index]
rho <- res[3,index]

C <- matrix(0,250,250)
for (i in 1:250) {
  for (j in 1:250) {
    C[i, j] <- K2(gamma,rho,train1x[i,],train1x[j,])
  }
}

C <- C + diag(250)

predict <- matrix(0,1,2500)
for (i in 1:nrow(testy)) {
  t <- matrix(0,1,250)
  for (j in 1:250) {
    t[j] <- K2(gamma,rho,train1x[j,],testx[i,])
  }
  predict[i] <- t%*%solve(C,train1y)
}

MSE3 <- sum((t(testy) - predict)^2)/2500
print(MSE3)
t2 <- proc.time()
print(t2-t1)

########## Gaussian process with hyperparameters (re-scaling) ##########

t1 <- proc.time()

trainxx <- train1x
testxx <- testx
trainxx[,1] <- trainxx[,1]/10
trainxx[,7] <- trainxx[,7]/10
testxx[,1] <- testxx[,1]/10
testxx[,7] <- testxx[,7]/10

res2 <- matrix(0,3,400)
num2 <- 1
for (gamma in seq(0.1,10,0.5)) {
  for (rho in seq(0.01,1,0.05)) {
    res2[1,num2] <- cv(gamma,rho,trainxx,train1y)
    res2[2,num2] <- gamma
    res2[3,num2] <- rho
    num2 <- num2 + 1
    cat(num2, res2[1,num2-1],res2[2,num2-1],res2[3,num2-1],"\n")
  }
}

write.table(res2, "./output2.txt", sep="\t")

index2 <- which(res2[1,] == min(res2[1,]))
gamma <- res2[,index2][2]
rho <- res2[,index2][3]

C <- matrix(0,250,250)
for (i in 1:250) {
  for (j in 1:250) {
    C[i, j] <- K2(gamma,rho,trainxx[i,],trainxx[j,])
  }
}

C <- C + diag(250)

predict2 <- matrix(0,1,2500)
for (i in 1:nrow(testy)) {
  t <- matrix(0,1,250)
  for (j in 1:250) {
    t[j] <- K2(gamma,rho,trainxx[j,],testxx[i,])
  }
  predict2[i] <- t%*%solve(C,train1y)
}

MSE4 <- sum((t(testy) - predict2)^2)/2500
print(MSE4)
t2 <- proc.time()
print(t2-t1)
```
- `output1.txt`
- `output2.txt`
